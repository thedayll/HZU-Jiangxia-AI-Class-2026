{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-30T02:48:29.703493Z",
     "start_time": "2026-01-30T02:48:23.062616Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T02:49:12.121688Z",
     "start_time": "2026-01-30T02:49:11.867456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义函数，使用指定数据创建张量\n",
    "def dm01():\n",
    "    # 场景1 标量 张量\n",
    "    t1 = torch.tensor(5)\n",
    "    print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 场景2 二维列表装换为张量\n",
    "    data = [[1, 2, 3], [4, 5, 6]]\n",
    "    t2 = torch.tensor(data)\n",
    "    print(f\"t2: {t2}, dtype: {type(t2)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 场景3 numpy数组转换为张量\n",
    "    data1 = np.random.randint(0, 10, (2, 3))\n",
    "    t3 = torch.tensor(data1, dtype=torch.float)\n",
    "    print(f\"t3: {t3}, dtype: {type(t3)}\")\n",
    "\n",
    "dm01()"
   ],
   "id": "ce5e306e0a2bb37c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: 5, dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t2: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t3: tensor([[3., 0., 4.],\n",
      "        [1., 8., 8.]]), dtype: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T02:49:40.812361Z",
     "start_time": "2026-01-30T02:49:40.800217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 场景1：torch.ones 和 torch.ones_like 创建一个全1张量\n",
    "t1 = torch.ones(2, 3)       # 创建一个2行3列的全1张量\n",
    "print(f\"t1: {t1}, type: {type(t1)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "t2 = torch.tensor([[1,2], [3,4], [5,6]])\n",
    "\n",
    "t3 = torch.ones_like(t2)\n",
    "print(f\"t3: {t3}, type: {type(t3)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "\n",
    "# 场景2：torch.zeros 和 torch.zeros_like 创建一个全0张量\n",
    "t4 = torch.zeros(2, 3)       # 创建一个2行3列的全1张量\n",
    "print(f\"t4: {t4}, type: {type(t4)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "t5 = torch.tensor([[1,2], [3,4], [5,6]])\n",
    "\n",
    "t6 = torch.zeros_like(t5)\n",
    "print(f\"t3: {t6}, type: {type(t6)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "# 场景3：torch.full 和 torch.full_like 创建一个指定值的张量\n",
    "t7 = torch.full((2, 3), 255)       # 创建一个2行3列的全5张量\n",
    "print(f\"t7: {t7}, type: {type(t7)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "t8 = torch.tensor([[1,2], [3,4], [5,6]])\n",
    "t9 = torch.full_like(t8, 255)\n",
    "print(f\"t9: {t9}, type: {type(t9)}\")"
   ],
   "id": "4acc1c87ed2dd7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t3: tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t4: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]), type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t3: tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t7: tensor([[255, 255, 255],\n",
      "        [255, 255, 255]]), type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t9: tensor([[255, 255],\n",
      "        [255, 255],\n",
      "        [255, 255]]), type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "创建线性张量和随机张量",
   "id": "49fd3dab23aa7ce4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T02:52:42.362095Z",
     "start_time": "2026-01-30T02:52:42.356745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 创建线性张量\n",
    "def dm01():\n",
    "    # 创建一个从0到9的步长为2的线性张量\n",
    "    t1 = torch.arange(0, 10, 2)\n",
    "    print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    t2 = torch.linspace(1, 10, 10)\n",
    "    print(f\"t2: {t2}, dtype: {type(t2)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "dm01()"
   ],
   "id": "307b3a7cef5ccc73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([0, 2, 4, 6, 8]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t2: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T02:52:45.634323Z",
     "start_time": "2026-01-30T02:52:45.629404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2- 定义函数，创建随机张量\n",
    "def dm02():\n",
    "    # 设置随机种子，确保每次生成的随机数相同\n",
    "    # torch.manual_seed(3)   # 设置随机固定种子\n",
    "    torch.initial_seed()     # 时间戳作为随机种子\n",
    "\n",
    "    # 创建随机张量\n",
    "    t1 = torch.rand(size=(2, 3))\n",
    "    print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 创建随机整数张量\n",
    "    t2 = torch.randint(0, 10, size=(3, 5))\n",
    "    print(f\"t2: {t2}, dtype: {type(t2)}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "dm02()"
   ],
   "id": "11658280e2b122d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[0.8022, 0.4205, 0.6663],\n",
      "        [0.6501, 0.6005, 0.4893]]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t2: tensor([[4, 6, 7, 6, 2],\n",
      "        [1, 3, 4, 2, 7],\n",
      "        [5, 4, 5, 8, 3]]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量的数据类型转换",
   "id": "47bcfcfb5b1fe771"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T02:55:02.897916Z",
     "start_time": "2026-01-30T02:55:02.888256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 场景1：直接创建指定类型张量\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "print(f\"t1: {t1}, 元素类型：{t1.dtype}, type: {type(t1)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "# 场景2：从已有张量转换类型\n",
    "# 1- 方式一：使用type()方法(优先使用这种)\n",
    "t2 = t1.type(torch.int32)\n",
    "print(f\"t2: {t2}, 元素类型：{t2.dtype}, type: {type(t2)}\")\n",
    "print('-' * 50)\n",
    "\n",
    "# 方式二：使用half()方法/double()方法等\n",
    "t3 = t2.half()          # 转换为float16类型\n",
    "t4 = t3.double()        # 转换为float64类型\n",
    "t5 = t4.float()         # 转换为float32类型\n",
    "print(f\"t3: {t3}, 元素类型：{t3.dtype}, type: {type(t3)}\")\n",
    "print(f\"t4: {t4}, 元素类型：{t4.dtype}, type: {type(t4)}\")\n",
    "print(f\"t5: {t5}, 元素类型：{t5.dtype}, type: {type(t5)}\")"
   ],
   "id": "213de0802e1da350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([1., 2., 3., 4., 5.]), 元素类型：torch.float32, type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t2: tensor([1, 2, 3, 4, 5], dtype=torch.int32), 元素类型：torch.int32, type: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t3: tensor([1., 2., 3., 4., 5.], dtype=torch.float16), 元素类型：torch.float16, type: <class 'torch.Tensor'>\n",
      "t4: tensor([1., 2., 3., 4., 5.], dtype=torch.float64), 元素类型：torch.float64, type: <class 'torch.Tensor'>\n",
      "t5: tensor([1., 2., 3., 4., 5.]), 元素类型：torch.float32, type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "张量与numpy数组的相互转换及基本运算\n",
    "    常用场景总结：\n",
    "\n",
    "        场景1：张量转为numpy数组\n",
    "            张量对象.numpy()                   共享内存\n",
    "            张量对象.numpy().copy()            不共享内存,链式编程写法\n",
    "        场景2：numpy数组转为张量\n",
    "            torch.from_numpy()                 共享内存\n",
    "            torch.tensor()                     不共享内存\n",
    "        场景3：从标量张量中提取其内容\n",
    "            从标量张量.item()\n",
    "\n",
    "        重点掌握的：\n",
    "            张量对象.numpy()\n",
    "            torch.tensor(nd数组)\n",
    "            从标量张量提取内容 标量张量.item()\n",
    "\n",
    "    张量基本运算总结：\n",
    "        add()、sub()、mul()、div()      张量的加减乘除运算\n",
    "        add_()、sub_()、mul_()、div_()  带下划线的会修改原张量（就地操作）"
   ],
   "id": "c77cc1b77fd3ffe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:01:34.899628Z",
     "start_time": "2026-01-30T03:01:34.887089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 场景1：张量转为numpy数组\n",
    "def dm01():\n",
    "    # 1- 创建张量\n",
    "    t1 = torch.tensor([1, 2, 3])\n",
    "    print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "\n",
    "    # 2- 张量转为numpy数组\n",
    "    nda = t1.numpy().copy()     # 不共享内存\n",
    "    n1 = t1.numpy()  # 共享内存\n",
    "\n",
    "    # 3- 打印结果，测试是否共享内存\n",
    "    n1[0] = 100\n",
    "    print(f\"nda: {nda}\")\n",
    "    print(f\"n1: {n1}\")\n",
    "    print(f\"t1: {t1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 场景2：numpy数组转为张量\n",
    "def dm02():\n",
    "    t1 = torch.tensor(np.array([1, 2, 3]))\n",
    "    print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 场景3：从标量张量中提取其内容\n",
    "def dm03():\n",
    "    t1 = torch.tensor(10)\n",
    "    print(f\"t1: {t1.item()}, dtype: {type(t1.item())}\")\n",
    "\n",
    "dm01(), dm02(), dm03()"
   ],
   "id": "348c7decd7cecee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([1, 2, 3]), dtype: <class 'torch.Tensor'>\n",
      "nda: [1 2 3]\n",
      "n1: [100   2   3]\n",
      "t1: tensor([100,   2,   3])\n",
      "--------------------------------------------------\n",
      "t1: tensor([1, 2, 3]), dtype: <class 'torch.Tensor'>\n",
      "--------------------------------------------------\n",
      "t1: 10, dtype: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:05:37.265346Z",
     "start_time": "2026-01-30T03:05:37.253969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "# 1- 张量的加法\n",
    "t3 = t1.add(t2)\n",
    "print(f\"t1 + t2: {t3}\")\n",
    "\n",
    "t4 = t1 + t2\n",
    "print(f\"t1 + t2: {t4}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2- 张量的减法\n",
    "t5 = t1.sub(t2)\n",
    "print(f\"t1 - t2: {t5}\")\n",
    "\n",
    "t6 = t1 - t2\n",
    "print(f\"t1 - t2: {t6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3- 张量的乘法\n",
    "t7 = t1.mul(t2)\n",
    "print(f\"t1 * t2: {t7}\")\n",
    "\n",
    "t8 = t1 * t2\n",
    "print(f\"t1 * t2: {t8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4- 张量的除法\n",
    "t9 = t1.div(t2)\n",
    "print(f\"t1 / t2: {t9}\")\n",
    "\n",
    "t10 = t1 / t2\n",
    "print(f\"t1 / t2: {t10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5- 取反\n",
    "t11 = -t1\n",
    "print(f\"-t1: {t11}\")\n",
    "\n",
    "t12 = t1.neg()\n",
    "print(f\"-t1: {t12}\")\n",
    "print(\"-\" * 50)"
   ],
   "id": "8979741559b9eb44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 + t2: tensor([5, 7, 9])\n",
      "t1 + t2: tensor([5, 7, 9])\n",
      "--------------------------------------------------\n",
      "t1 - t2: tensor([-3, -3, -3])\n",
      "t1 - t2: tensor([-3, -3, -3])\n",
      "--------------------------------------------------\n",
      "t1 * t2: tensor([ 4, 10, 18])\n",
      "t1 * t2: tensor([ 4, 10, 18])\n",
      "--------------------------------------------------\n",
      "t1 / t2: tensor([0.2500, 0.4000, 0.5000])\n",
      "t1 / t2: tensor([0.2500, 0.4000, 0.5000])\n",
      "--------------------------------------------------\n",
      "-t1: tensor([-1, -2, -3])\n",
      "-t1: tensor([-1, -2, -3])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    张量的点乘和矩阵乘法\n",
    "    点乘：\n",
    "        api：\n",
    "            t1 * t2\n",
    "            t1.mul(t2)\n",
    "\n",
    "    矩阵乘法：\n",
    "        api:\n",
    "            t1 @ t2\n",
    "            t1.matmul(t2)\n",
    "            t1.dot(t2)"
   ],
   "id": "7284579c82a5164"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:08:37.955185Z",
     "start_time": "2026-01-30T03:08:37.948854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dm01():\n",
    "    # 1- 创建两个个两行三列的张量\n",
    "    t01 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    print(f\"t01: {t01}\")\n",
    "\n",
    "    t02 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    print(f\"t02: {t02}\")\n",
    "\n",
    "    # 2- 点乘\n",
    "    # t03 = t01.mul(t02)\n",
    "    t03 = t01 * t02\n",
    "    print(f\"t01 * t02: {t03}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "dm01()"
   ],
   "id": "85a7183b3dc862f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t01: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t02: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t01 * t02: tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:08:40.727954Z",
     "start_time": "2026-01-30T03:08:40.722004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义函数，矩阵乘法\n",
    "def dm02():\n",
    "    # 1- 创建2x3的张量\n",
    "    t04 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    print(f\"t01: {t04}\")\n",
    "\n",
    "    # 2- 创建3x2的张量\n",
    "    t05 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "    print(f\"t02: {t05}\")\n",
    "\n",
    "    # 3- 矩阵乘法\n",
    "    t06 = t04.matmul(t05)\n",
    "    # t03 = t03 @ t04\n",
    "    print(f\"t01 * t02: {t06}\")\n",
    "\n",
    "dm02()"
   ],
   "id": "317b60cdcac0ed59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t01: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t02: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "t01 * t02: tensor([[22, 28],\n",
      "        [49, 64]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量常用运算函数",
   "id": "b8e7131af5e3373a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:10:50.801375Z",
     "start_time": "2026-01-30T03:10:50.780861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 定义2x3张量\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float)\n",
    "\n",
    "# 2- 按不同维度计算张量的值\n",
    "# 2.1- 求和\n",
    "print(f\"t1.sum(): {t1.sum()}\")                  # 整体求和\n",
    "print(f\"t1.sum(dim=0): {t1.sum(dim=0)}\")        # 按行求和\n",
    "print(f\"t1.sum(dim=1): {t1.sum(dim=1)}\")        # 按列求和\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2.2 求最大值\n",
    "print(f\"t1.max(): {t1.max()}\")\n",
    "print(f\"t1.max(dim=0): {t1.max(dim=0)}\")\n",
    "print(f\"t1.max(dim=1): {t1.max(dim=1)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2.3 求最小值\n",
    "print(f\"t1.min(): {t1.min()}\")\n",
    "print(f\"t1.min(dim=0): {t1.min(dim=0)}\")\n",
    "print(f\"t1.min(dim=1): {t1.min(dim=1)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2.4 求均值\n",
    "print(f\"t1.mean(): {t1.mean()}\")\n",
    "print(f\"t1.mean(dim=0): {t1.mean(dim=0)}\")\n",
    "print(f\"t1.mean(dim=1): {t1.mean(dim=1)}\")"
   ],
   "id": "ef3f5dbba577d7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1.sum(): 21.0\n",
      "t1.sum(dim=0): tensor([5., 7., 9.])\n",
      "t1.sum(dim=1): tensor([ 6., 15.])\n",
      "--------------------------------------------------\n",
      "t1.max(): 6.0\n",
      "t1.max(dim=0): torch.return_types.max(\n",
      "values=tensor([4., 5., 6.]),\n",
      "indices=tensor([1, 1, 1]))\n",
      "t1.max(dim=1): torch.return_types.max(\n",
      "values=tensor([3., 6.]),\n",
      "indices=tensor([2, 2]))\n",
      "--------------------------------------------------\n",
      "t1.min(): 1.0\n",
      "t1.min(dim=0): torch.return_types.min(\n",
      "values=tensor([1., 2., 3.]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "t1.min(dim=1): torch.return_types.min(\n",
      "values=tensor([1., 4.]),\n",
      "indices=tensor([0, 0]))\n",
      "--------------------------------------------------\n",
      "t1.mean(): 3.5\n",
      "t1.mean(dim=0): tensor([2.5000, 3.5000, 4.5000])\n",
      "t1.mean(dim=1): tensor([2., 5.])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "张量的索引操作",
   "id": "5f235a4bed011285"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:11:53.556061Z",
     "start_time": "2026-01-30T03:11:53.497991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 设置随机种子\n",
    "torch.manual_seed(24)\n",
    "\n",
    "# 2- 创建张量\n",
    "t1 = torch.randint(1, 10, (5, 5))\n",
    "print(f\"t1: {t1}\")\n",
    "print('-' * 50)\n",
    "\n",
    "# 3- 索引操作\n",
    "# 场景1：简单行列索引\n",
    "print(t1[1])        # 获取第二行\n",
    "print(t1[1, :])     # 等价于上面一行\n",
    "print('-' * 50)\n",
    "print(t1[:, 1])     # 获取第二列\n",
    "print('-' * 50)\n",
    "\n",
    "# 场景2：列表索引\n",
    "# 1) 获取(0, 1)和(1, 2)两个位置的元素\n",
    "print(t1[[0, 1], [1, 2]])\n",
    "\n",
    "# 2) 获取(1, 2)和(3, 4)两个位置的元素   注：第一个列表表示行索引，第二个列表表示列索引\n",
    "print(t1[[1, 3], [2, 4]])\n",
    "print('-' * 50)\n",
    "\n",
    "# 3) 获取第0、1行的第1、2列元素\n",
    "print(t1[[0, 1], 1:3])\n",
    "print('-' * 50)\n",
    "print(t1[[[0], [1]], [1, 2]])\n",
    "print('-' * 50)\n",
    "\n",
    "# 场景3：范围索引\n",
    "# 1) 获取前三行，前两列的数据\n",
    "print(t1[:3, :2])\n",
    "\n",
    "# 2) 获取第2到最后一行，前两列数据\n",
    "print(t1[2:, :2])\n",
    "\n",
    "# 3) 获取所有奇数行，偶数列的数据\n",
    "print(t1[1::2, ::2])\n",
    "print('-' * 50)\n",
    "\n",
    "# 场景4：布尔索引\n",
    "# 1) 获取第三列大于5数据的行\n",
    "print(t1[t1[:, 2] > 5])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2) 获取第二行大于5的列数据\n",
    "# 注意明确要取的是行、行元素、还是列\n",
    "\"\"\"\n",
    "    t1[t1[1, :] > 5]: 把返回的列索引列表作为行索引传入, 获取到的是行\n",
    "    t1[1, t1[1, :] > 5]: 把返回的列索引列表作为列索引传入, 第一个参数传入的是指定行, 获取值指定行满足条件的值\n",
    "    t1[: ,t1[1, :] > 5]: 把返回的列索引列表作为列索引传入, 第一个参数为: , 获取值所有行中满足条件的列\n",
    "\"\"\"\n",
    "print(t1[: ,t1[1, :] > 5])\n",
    "print(t1[t1[1, :] > 5])\n",
    "print(t1[1, t1[1, :] > 5])\n",
    "print(t1[1, :] > 5)\n",
    "print('-' * 50)\n",
    "\n",
    "\n",
    "# 场景5：多维索引\n",
    "# 创建三维向量\n",
    "t2 = torch.randint(1, 10, (2, 3, 4))\n",
    "print(f't2: {t2}')\n",
    "print('-' * 50)\n",
    "\n",
    "# 1) 获取0轴上的第一个数据\n",
    "print(t2[0, :, :])\n",
    "\n",
    "# 2) 获取1轴上的第一个数据\n",
    "print(t2[:, 0, :])\n",
    "\n",
    "# 3) 获取2轴上的第一个数据\n",
    "print(t2[:, :, 0])"
   ],
   "id": "97e9b1675271b1e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[6, 9, 9, 2, 8],\n",
      "        [7, 8, 5, 8, 4],\n",
      "        [7, 4, 3, 9, 3],\n",
      "        [6, 1, 4, 2, 8],\n",
      "        [1, 2, 5, 7, 4]])\n",
      "--------------------------------------------------\n",
      "tensor([7, 8, 5, 8, 4])\n",
      "tensor([7, 8, 5, 8, 4])\n",
      "--------------------------------------------------\n",
      "tensor([9, 8, 4, 1, 2])\n",
      "--------------------------------------------------\n",
      "tensor([9, 5])\n",
      "tensor([5, 8])\n",
      "--------------------------------------------------\n",
      "tensor([[9, 9],\n",
      "        [8, 5]])\n",
      "--------------------------------------------------\n",
      "tensor([[9, 9],\n",
      "        [8, 5]])\n",
      "--------------------------------------------------\n",
      "tensor([[6, 9],\n",
      "        [7, 8],\n",
      "        [7, 4]])\n",
      "tensor([[7, 4],\n",
      "        [6, 1],\n",
      "        [1, 2]])\n",
      "tensor([[7, 5, 4],\n",
      "        [6, 4, 8]])\n",
      "--------------------------------------------------\n",
      "tensor([[6, 9, 9, 2, 8]])\n",
      "--------------------------------------------------\n",
      "tensor([[6, 9, 2],\n",
      "        [7, 8, 8],\n",
      "        [7, 4, 9],\n",
      "        [6, 1, 2],\n",
      "        [1, 2, 7]])\n",
      "tensor([[6, 9, 9, 2, 8],\n",
      "        [7, 8, 5, 8, 4],\n",
      "        [6, 1, 4, 2, 8]])\n",
      "tensor([7, 8, 8])\n",
      "tensor([ True,  True, False,  True, False])\n",
      "--------------------------------------------------\n",
      "t2: tensor([[[3, 4, 6, 5],\n",
      "         [8, 8, 8, 3],\n",
      "         [4, 9, 6, 7]],\n",
      "\n",
      "        [[2, 8, 8, 5],\n",
      "         [6, 4, 2, 2],\n",
      "         [2, 7, 9, 4]]])\n",
      "--------------------------------------------------\n",
      "tensor([[3, 4, 6, 5],\n",
      "        [8, 8, 8, 3],\n",
      "        [4, 9, 6, 7]])\n",
      "tensor([[3, 4, 6, 5],\n",
      "        [2, 8, 8, 5]])\n",
      "tensor([[3, 8, 4],\n",
      "        [2, 6, 2]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "张量的形状操作\n",
    "\n",
    "    API总结：\n",
    "        张量的形状操作：\n",
    "                    reshape(): 在不改变张量内容的前提下，对其形状做出改变\n",
    "                    squeeze(): 降维，删除所有为1的维度\n",
    "                    unsqueeze(): 升维，在指定位置增加一个维度，值为1\n",
    "                    transpose(): 一次只能交换2个维度\n",
    "                    permute(): 一次同时交换多个维度\n",
    "                    contiguous()： 返回一个内存连续的张量拷贝，保证张量在内存中是按行优先存储的\n",
    "                    view()： 在不改变张量内容的前提下，对其形状做出改变，要求张量在内存中是连续存储的"
   ],
   "id": "c96d896260cc8434"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:14:42.108760Z",
     "start_time": "2026-01-30T03:14:42.102680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 定义函数，应用reshape()函数\n",
    "def dm01():\n",
    "    # 创建2x3张量\n",
    "    t1 = torch.randint(1, 10, (2, 3))\n",
    "    print(f\"t1: {t1}, shape: {t1.shape}, row: {t1.shape[0]}, colomn: {t1.shape[1]}\")\n",
    "\n",
    "    t2 = t1.reshape(3, 2)\n",
    "    print(f\"t2: {t2}, shape: {t2.shape}, row: {t2.shape[0]}, colomn: {t2.shape[1]}\")\n",
    "\n",
    "dm01()"
   ],
   "id": "4a3110644ebacd41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[2, 6, 3],\n",
      "        [2, 3, 7]]), shape: torch.Size([2, 3]), row: 2, colomn: 3\n",
      "t2: tensor([[2, 6],\n",
      "        [3, 2],\n",
      "        [3, 7]]), shape: torch.Size([3, 2]), row: 3, colomn: 2\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:15:06.354390Z",
     "start_time": "2026-01-30T03:15:06.347381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2- 创建函数，应用unsqueeze()函数和squeeze()函数\n",
    "def dm02():\n",
    "    t1 = torch.randint(1, 10, size=(2, 3))\n",
    "    print(f\"t1: {t1}, shape: {t1.shape}, row: {t1.shape[0]}, colomn: {t1.shape[1]}\")\n",
    "\n",
    "    # 在0维度增加一个维度\n",
    "    t2 = t1.unsqueeze(0)\n",
    "    print(f\"t2: {t2}, shape: {t2.shape}\")      # (1, 2, 3)\n",
    "\n",
    "    # 在1维度增加一个维度\n",
    "    t3 = t1.unsqueeze(1)\n",
    "    print(f\"t3: {t3}, shape: {t3.shape}\")       # (2, 1, 3)\n",
    "\n",
    "    # 在2维度增加一个维度\n",
    "    t4 = t1.unsqueeze(2)\n",
    "    print(f\"t4: {t4}, shape: {t4.shape}\")       # (2, 3, 1)\n",
    "\n",
    "    # # 在3维度增加一个维度\n",
    "    # t5 = t1.unsqueeze(3)                        # 报错\n",
    "    # print(f\"t5: {t5}, shape: {t5.shape}\")       # (2, 3, ?, 1)\n",
    "\n",
    "    # 删除所有为1的维度\n",
    "    t6 = torch.randint(1, 10, size=(2, 1, 3, 1, 1))\n",
    "    print(f\"t6: {t6}, shape: {t6.shape}\")\n",
    "\n",
    "    t7 = t6.squeeze()\n",
    "    print(f\"t7: {t7}, shape: {t7.shape}\")\n",
    "\n",
    "dm02()"
   ],
   "id": "f6d11102b77230cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[5, 1, 7],\n",
      "        [9, 2, 3]]), shape: torch.Size([2, 3]), row: 2, colomn: 3\n",
      "t2: tensor([[[5, 1, 7],\n",
      "         [9, 2, 3]]]), shape: torch.Size([1, 2, 3])\n",
      "t3: tensor([[[5, 1, 7]],\n",
      "\n",
      "        [[9, 2, 3]]]), shape: torch.Size([2, 1, 3])\n",
      "t4: tensor([[[5],\n",
      "         [1],\n",
      "         [7]],\n",
      "\n",
      "        [[9],\n",
      "         [2],\n",
      "         [3]]]), shape: torch.Size([2, 3, 1])\n",
      "t6: tensor([[[[[1]],\n",
      "\n",
      "          [[9]],\n",
      "\n",
      "          [[6]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[8]],\n",
      "\n",
      "          [[5]],\n",
      "\n",
      "          [[7]]]]]), shape: torch.Size([2, 1, 3, 1, 1])\n",
      "t7: tensor([[1, 9, 6],\n",
      "        [8, 5, 7]]), shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:15:19.907793Z",
     "start_time": "2026-01-30T03:15:19.899793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义函数，应用transpose()函数和permute()函数\n",
    "def dm03():\n",
    "    t1 = torch.randint(1, 10, size=(2, 3, 4))\n",
    "    print(f\"t1: {t1}, shape: {t1.shape}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 交换0和1两个维度\n",
    "    t2 = t1.transpose(0, 1)\n",
    "    print(f\"t2: {t2}, shape: {t2.shape}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 交换0和2两个维度\n",
    "    t3 = t1.transpose(0, 2)\n",
    "    print(f\"t3: {t3}, shape: {t3.shape}\")\n",
    "    t4 = t1.transpose(0, -1)            # 等价与t3\n",
    "    print(f\"t4: {t4}, shape: {t4.shape}\")\n",
    "\n",
    "    # (2, 3, 4) -> (3, 4, 2)\n",
    "    t5 = t1.permute(1, 2, 0)\n",
    "    print(f\"t5: {t5}, shape: {t5.shape}\")\n",
    "\n",
    "dm03()"
   ],
   "id": "907526837c8ba278",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[[5, 8, 5, 9],\n",
      "         [5, 6, 2, 1],\n",
      "         [3, 9, 8, 7]],\n",
      "\n",
      "        [[1, 6, 5, 3],\n",
      "         [8, 3, 8, 8],\n",
      "         [1, 8, 9, 4]]]), shape: torch.Size([2, 3, 4])\n",
      "--------------------------------------------------\n",
      "t2: tensor([[[5, 8, 5, 9],\n",
      "         [1, 6, 5, 3]],\n",
      "\n",
      "        [[5, 6, 2, 1],\n",
      "         [8, 3, 8, 8]],\n",
      "\n",
      "        [[3, 9, 8, 7],\n",
      "         [1, 8, 9, 4]]]), shape: torch.Size([3, 2, 4])\n",
      "--------------------------------------------------\n",
      "t3: tensor([[[5, 1],\n",
      "         [5, 8],\n",
      "         [3, 1]],\n",
      "\n",
      "        [[8, 6],\n",
      "         [6, 3],\n",
      "         [9, 8]],\n",
      "\n",
      "        [[5, 5],\n",
      "         [2, 8],\n",
      "         [8, 9]],\n",
      "\n",
      "        [[9, 3],\n",
      "         [1, 8],\n",
      "         [7, 4]]]), shape: torch.Size([4, 3, 2])\n",
      "t4: tensor([[[5, 1],\n",
      "         [5, 8],\n",
      "         [3, 1]],\n",
      "\n",
      "        [[8, 6],\n",
      "         [6, 3],\n",
      "         [9, 8]],\n",
      "\n",
      "        [[5, 5],\n",
      "         [2, 8],\n",
      "         [8, 9]],\n",
      "\n",
      "        [[9, 3],\n",
      "         [1, 8],\n",
      "         [7, 4]]]), shape: torch.Size([4, 3, 2])\n",
      "t5: tensor([[[5, 1],\n",
      "         [8, 6],\n",
      "         [5, 5],\n",
      "         [9, 3]],\n",
      "\n",
      "        [[5, 8],\n",
      "         [6, 3],\n",
      "         [2, 8],\n",
      "         [1, 8]],\n",
      "\n",
      "        [[3, 1],\n",
      "         [9, 8],\n",
      "         [8, 9],\n",
      "         [7, 4]]]), shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:15:41.137488Z",
     "start_time": "2026-01-30T03:15:41.131499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dm04():\n",
    "    t1 = torch.randint(1, 10, size=(2, 3))\n",
    "    print(f\"t1: {t1}, shape: {t1.shape}\")\n",
    "\n",
    "    # 判断张量是否连续存储, 即张量中的顺序 和 内存中的存储顺序一致\n",
    "    print(t1.is_contiguous())\n",
    "\n",
    "    t2 = t1.view(3, 2)\n",
    "    print(f\"t2: {t2}, shape: {t2.shape}\")\n",
    "    print(t2.is_contiguous())\n",
    "\n",
    "    t3 = t1.transpose(0, 1)\n",
    "    print(f\"t3: {t3}, shape: {t3.shape}\")\n",
    "    print(t3.is_contiguous())\n",
    "\n",
    "    # t4 = t3.view(2, 3)      # 报错\n",
    "    # print(f\"t4: {t4}, shape: {t4.shape}\")\n",
    "    # print(t4.is_contiguous())\n",
    "\n",
    "    # 将t3转换为连续存储\n",
    "    t4 = t3.contiguous()\n",
    "    print(f\"t4: {t4}, shape: {t4.shape}\")\n",
    "    print(t4.is_contiguous())\n",
    "\n",
    "dm04()"
   ],
   "id": "3511a3588fee8951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[3, 8, 6],\n",
      "        [1, 4, 6]]), shape: torch.Size([2, 3])\n",
      "True\n",
      "t2: tensor([[3, 8],\n",
      "        [6, 1],\n",
      "        [4, 6]]), shape: torch.Size([3, 2])\n",
      "True\n",
      "t3: tensor([[3, 1],\n",
      "        [8, 4],\n",
      "        [6, 6]]), shape: torch.Size([3, 2])\n",
      "False\n",
      "t4: tensor([[3, 1],\n",
      "        [8, 4],\n",
      "        [6, 6]]), shape: torch.Size([3, 2])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "张量的拼接与堆叠\n",
    "\n",
    "    API总结：\n",
    "        cat(): 不改变维度数，除了拼接的维度上，其他维度必须相同，\n",
    "        stack(): 改变维度数，所有维度必须相同"
   ],
   "id": "f9306a63faef8455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:18:16.378885Z",
     "start_time": "2026-01-30T03:18:16.373666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 创建两个张量\n",
    "t1 = torch.randint(1, 10, (2, 3))\n",
    "print(f\"t1: {t1}, shape: {t1.shape}\")\n",
    "\n",
    "t2 = torch.randint(1, 10, (2, 3))\n",
    "print(f\"t2: {t2}, shape: {t2.shape}\")"
   ],
   "id": "767ab9add80fb80d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[5, 6, 8],\n",
      "        [8, 6, 5]]), shape: torch.Size([2, 3])\n",
      "t2: tensor([[7, 3, 8],\n",
      "        [4, 8, 4]]), shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:18:11.573130Z",
     "start_time": "2026-01-30T03:18:11.568129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2- cat()函数拼接张量\n",
    "t3 = torch.cat([t1, t2], dim=0)\n",
    "print(f\"t3: {t3}, shape: {t3.shape}\")\n",
    "\n",
    "t4 = torch.cat([t1, t2], dim=1)\n",
    "print(f\"t4: {t4}, shape: {t4.shape}\")"
   ],
   "id": "b0a6e7cdf34343a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3: tensor([[2, 3, 5],\n",
      "        [3, 7, 2],\n",
      "        [8, 3, 6],\n",
      "        [8, 6, 3]]), shape: torch.Size([4, 3])\n",
      "t4: tensor([[2, 3, 5, 8, 3, 6],\n",
      "        [3, 7, 2, 8, 6, 3]]), shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:18:04.679951Z",
     "start_time": "2026-01-30T03:18:04.674820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3- stack()函数拼接张量\n",
    "t5 = torch.stack([t1, t2], dim=0)\n",
    "print(f\"t5: {t5}, shape: {t5.shape}\")\n",
    "\n",
    "t6 = torch.stack([t1, t2], dim=1)\n",
    "print(f\"t6: {t6}, shape: {t6.shape}\")\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "print(f\"t7: {t7}, shape: {t7.shape}\")"
   ],
   "id": "7e21ebf902a49746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5: tensor([[[2, 3, 5],\n",
      "         [3, 7, 2]],\n",
      "\n",
      "        [[8, 3, 6],\n",
      "         [8, 6, 3]]]), shape: torch.Size([2, 2, 3])\n",
      "t6: tensor([[[2, 3, 5],\n",
      "         [8, 3, 6]],\n",
      "\n",
      "        [[3, 7, 2],\n",
      "         [8, 6, 3]]]), shape: torch.Size([2, 2, 3])\n",
      "t7: tensor([[[2, 8],\n",
      "         [3, 3],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 8],\n",
      "         [7, 6],\n",
      "         [2, 3]]]), shape: torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "自动微分模块\n",
    "\n",
    "    权重更新公式:\n",
    "        w新 = w旧 - 学习率 * 梯度\n",
    "        梯度 = 损失函数的导数"
   ],
   "id": "d599d7b325589865"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:23:25.760304Z",
     "start_time": "2026-01-30T03:23:25.755132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 定义变量，记录初始权重参数\n",
    "w = torch.tensor(10, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# 2- 定义损失函数\n",
    "loss = 2 * w ** 2\n",
    "\n",
    "# # 3- 打印梯度函数类型(了解)\n",
    "# print(f\"损失函数的类型为: {type(loss.grad_fn)}\")\n",
    "\n",
    "# 4- 反向传播，计算梯度 => 损失函数的导数\n",
    "loss.sum().backward()    # 保证loss是一个标量\n",
    "\n",
    "# 5- 带入权重更新公式，计算新的权重参数\n",
    "w.data = w.data - 0.01 * w.grad\n",
    "\n",
    "# 6- 打印最终结果\n",
    "print(f\"最终结果为: {w}\")"
   ],
   "id": "be1b9945425ff5f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终结果为: 9.600000381469727\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "自动微分模块demo, 使用梯度下降法，迭代100次，求最优解",
   "id": "67372e54ede3479e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:24:07.782264Z",
     "start_time": "2026-01-30T03:24:07.767721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 初始化权重参数和定义损失函数\n",
    "w = torch.tensor(10, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "loss = w ** 2 + 20\n",
    "\n",
    "# 2- 利用梯度下降法，迭代100次，求最优解\n",
    "print(f\"开始权重初始值为: {w}, (0.01 * w.grad): 无, loss: {loss}\")\n",
    "\n",
    "# 迭代100次，求最优解\n",
    "for i in range(1, 101):\n",
    "    # 2.1- 正向计算（前向传播）\n",
    "    loss = w ** 2 + 20\n",
    "\n",
    "    # 2.2- 梯度清零\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "\n",
    "    # 2.3- 反向传播，计算梯度\n",
    "    loss.sum().backward()\n",
    "\n",
    "    # 2.4- 权重更新\n",
    "    w.data = w.data - 0.01 * w.grad\n",
    "\n",
    "    # 2.5- 打印每次迭代结果\n",
    "    print(f\"第{i}次迭代，权重更新为: {w}, (0.01 * w.grad): {0.01 * w.grad}, loss: {loss: .5f}\")\n",
    "\n",
    "# 3- 打印最终结果\n",
    "print(f\"最终结果为: {w}, 梯度: {w.grad}, loss: {loss: .5f}\")"
   ],
   "id": "d47a6ab979ae0c2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始权重初始值为: 10.0, (0.01 * w.grad): 无, loss: 120.0\n",
      "第1次迭代，权重更新为: 9.800000190734863, (0.01 * w.grad): 0.19999998807907104, loss:  120.00000\n",
      "第2次迭代，权重更新为: 9.604000091552734, (0.01 * w.grad): 0.19599999487400055, loss:  116.04000\n",
      "第3次迭代，权重更新为: 9.411920547485352, (0.01 * w.grad): 0.19207999110221863, loss:  112.23682\n",
      "第4次迭代，权重更新为: 9.223682403564453, (0.01 * w.grad): 0.18823841214179993, loss:  108.58425\n",
      "第5次迭代，权重更新为: 9.03920841217041, (0.01 * w.grad): 0.1844736486673355, loss:  105.07632\n",
      "第6次迭代，权重更新为: 8.858424186706543, (0.01 * w.grad): 0.1807841658592224, loss:  101.70729\n",
      "第7次迭代，权重更新为: 8.681255340576172, (0.01 * w.grad): 0.17716847360134125, loss:  98.47168\n",
      "第8次迭代，权重更新为: 8.507630348205566, (0.01 * w.grad): 0.17362509667873383, loss:  95.36420\n",
      "第9次迭代，权重更新为: 8.337477684020996, (0.01 * w.grad): 0.17015260457992554, loss:  92.37978\n",
      "第10次迭代，权重更新为: 8.170727729797363, (0.01 * w.grad): 0.16674955189228058, loss:  89.51353\n",
      "第11次迭代，权重更新为: 8.007312774658203, (0.01 * w.grad): 0.16341455280780792, loss:  86.76079\n",
      "第12次迭代，权重更新为: 7.847166538238525, (0.01 * w.grad): 0.16014625132083893, loss:  84.11706\n",
      "第13次迭代，权重更新为: 7.690223217010498, (0.01 * w.grad): 0.15694332122802734, loss:  81.57802\n",
      "第14次迭代，权重更新为: 7.536418914794922, (0.01 * w.grad): 0.1538044661283493, loss:  79.13953\n",
      "第15次迭代，权重更新为: 7.385690689086914, (0.01 * w.grad): 0.15072837471961975, loss:  76.79761\n",
      "第16次迭代，权重更新为: 7.237977027893066, (0.01 * w.grad): 0.1477138102054596, loss:  74.54843\n",
      "第17次迭代，权重更新为: 7.093217372894287, (0.01 * w.grad): 0.14475953578948975, loss:  72.38831\n",
      "第18次迭代，权重更新为: 6.951353073120117, (0.01 * w.grad): 0.1418643444776535, loss:  70.31374\n",
      "第19次迭代，权重更新为: 6.812325954437256, (0.01 * w.grad): 0.13902705907821655, loss:  68.32130\n",
      "第20次迭代，权重更新为: 6.676079273223877, (0.01 * w.grad): 0.13624651730060577, loss:  66.40778\n",
      "第21次迭代，权重更新为: 6.542557716369629, (0.01 * w.grad): 0.13352158665657043, loss:  64.57004\n",
      "第22次迭代，权重更新为: 6.411706447601318, (0.01 * w.grad): 0.130851149559021, loss:  62.80506\n",
      "第23次迭代，权重更新为: 6.283472537994385, (0.01 * w.grad): 0.1282341331243515, loss:  61.10998\n",
      "第24次迭代，权重更新为: 6.157803058624268, (0.01 * w.grad): 0.1256694495677948, loss:  59.48203\n",
      "第25次迭代，权重更新为: 6.034646987915039, (0.01 * w.grad): 0.12315605580806732, loss:  57.91854\n",
      "第26次迭代，权重更新为: 5.913954257965088, (0.01 * w.grad): 0.12069293856620789, loss:  56.41697\n",
      "第27次迭代，权重更新为: 5.795675277709961, (0.01 * w.grad): 0.11827908456325531, loss:  54.97485\n",
      "第28次迭代，权重更新为: 5.67976188659668, (0.01 * w.grad): 0.1159135028719902, loss:  53.58985\n",
      "第29次迭代，权重更新为: 5.566166877746582, (0.01 * w.grad): 0.11359523236751556, loss:  52.25970\n",
      "第30次迭代，权重更新为: 5.454843521118164, (0.01 * w.grad): 0.11132333427667618, loss:  50.98222\n",
      "第31次迭代，权重更新为: 5.3457465171813965, (0.01 * w.grad): 0.10909686982631683, loss:  49.75532\n",
      "第32次迭代，权重更新为: 5.238831520080566, (0.01 * w.grad): 0.1069149300456047, loss:  48.57700\n",
      "第33次迭代，权重更新为: 5.134054660797119, (0.01 * w.grad): 0.10477662831544876, loss:  47.44536\n",
      "第34次迭代，权重更新为: 5.031373500823975, (0.01 * w.grad): 0.10268109291791916, loss:  46.35852\n",
      "第35次迭代，权重更新为: 4.930746078491211, (0.01 * w.grad): 0.10062746703624725, loss:  45.31472\n",
      "第36次迭代，权重更新为: 4.832131385803223, (0.01 * w.grad): 0.09861491620540619, loss:  44.31226\n",
      "第37次迭代，权重更新为: 4.7354888916015625, (0.01 * w.grad): 0.0966426283121109, loss:  43.34949\n",
      "第38次迭代，权重更新为: 4.6407790184021, (0.01 * w.grad): 0.09470977634191513, loss:  42.42485\n",
      "第39次迭代，权重更新为: 4.547963619232178, (0.01 * w.grad): 0.0928155779838562, loss:  41.53683\n",
      "第40次迭代，权重更新为: 4.457004547119141, (0.01 * w.grad): 0.09095927327871323, loss:  40.68398\n",
      "第41次迭代，权重更新为: 4.367864608764648, (0.01 * w.grad): 0.08914008736610413, loss:  39.86489\n",
      "第42次迭代，权重更新为: 4.2805070877075195, (0.01 * w.grad): 0.0873572900891304, loss:  39.07824\n",
      "第43次迭代，权重更新为: 4.194897174835205, (0.01 * w.grad): 0.08561013638973236, loss:  38.32274\n",
      "第44次迭代，权重更新为: 4.11099910736084, (0.01 * w.grad): 0.08389794081449509, loss:  37.59716\n",
      "第45次迭代，权重更新为: 4.028779029846191, (0.01 * w.grad): 0.08221998065710068, loss:  36.90031\n",
      "第46次迭代，权重更新为: 3.9482035636901855, (0.01 * w.grad): 0.08057557791471481, loss:  36.23106\n",
      "第47次迭代，权重更新为: 3.869239568710327, (0.01 * w.grad): 0.07896406948566437, loss:  35.58831\n",
      "第48次迭代，权重更新为: 3.7918548583984375, (0.01 * w.grad): 0.07738479226827621, loss:  34.97102\n",
      "第49次迭代，权重更新为: 3.716017723083496, (0.01 * w.grad): 0.07583709806203842, loss:  34.37816\n",
      "第50次迭代，权重更新为: 3.641697406768799, (0.01 * w.grad): 0.07432035356760025, loss:  33.80879\n",
      "第51次迭代，权重更新为: 3.5688633918762207, (0.01 * w.grad): 0.07283394783735275, loss:  33.26196\n",
      "第52次迭代，权重更新为: 3.497486114501953, (0.01 * w.grad): 0.07137726992368698, loss:  32.73679\n",
      "第53次迭代，权重更新为: 3.4275364875793457, (0.01 * w.grad): 0.06994972378015518, loss:  32.23241\n",
      "第54次迭代，权重更新为: 3.358985662460327, (0.01 * w.grad): 0.0685507282614708, loss:  31.74801\n",
      "第55次迭代，权重更新为: 3.2918059825897217, (0.01 * w.grad): 0.06717970967292786, loss:  31.28278\n",
      "第56次迭代，权重更新为: 3.2259697914123535, (0.01 * w.grad): 0.0658361166715622, loss:  30.83599\n",
      "第57次迭代，权重更新为: 3.1614503860473633, (0.01 * w.grad): 0.06451939791440964, loss:  30.40688\n",
      "第58次迭代，权重更新为: 3.0982213020324707, (0.01 * w.grad): 0.06322900950908661, loss:  29.99477\n",
      "第59次迭代，权重更新为: 3.036256790161133, (0.01 * w.grad): 0.061964426189661026, loss:  29.59898\n",
      "第60次迭代，权重更新为: 2.975531578063965, (0.01 * w.grad): 0.0607251338660717, loss:  29.21885\n",
      "第61次迭代，权重更新为: 2.9160208702087402, (0.01 * w.grad): 0.05951062962412834, loss:  28.85379\n",
      "第62次迭代，权重更新为: 2.8577003479003906, (0.01 * w.grad): 0.058320414274930954, loss:  28.50318\n",
      "第63次迭代，权重更新为: 2.800546407699585, (0.01 * w.grad): 0.057154007256031036, loss:  28.16645\n",
      "第64次迭代，权重更新为: 2.744535446166992, (0.01 * w.grad): 0.05601092800498009, loss:  27.84306\n",
      "第65次迭代，权重更新为: 2.6896448135375977, (0.01 * w.grad): 0.0548907071352005, loss:  27.53247\n",
      "第66次迭代，权重更新为: 2.6358518600463867, (0.01 * w.grad): 0.05379289388656616, loss:  27.23419\n",
      "第67次迭代，权重更新为: 2.583134889602661, (0.01 * w.grad): 0.05271703749895096, loss:  26.94772\n",
      "第68次迭代，权重更新为: 2.5314722061157227, (0.01 * w.grad): 0.05166269838809967, loss:  26.67259\n",
      "第69次迭代，权重更新为: 2.4808428287506104, (0.01 * w.grad): 0.05062944442033768, loss:  26.40835\n",
      "第70次迭代，权重更新为: 2.4312260150909424, (0.01 * w.grad): 0.04961685463786125, loss:  26.15458\n",
      "第71次迭代，权重更新为: 2.382601499557495, (0.01 * w.grad): 0.048624519258737564, loss:  25.91086\n",
      "第72次迭代，权重更新为: 2.334949493408203, (0.01 * w.grad): 0.04765202850103378, loss:  25.67679\n",
      "第73次迭代，权重更新为: 2.28825044631958, (0.01 * w.grad): 0.04669898748397827, loss:  25.45199\n",
      "第74次迭代，权重更新为: 2.242485523223877, (0.01 * w.grad): 0.04576500877737999, loss:  25.23609\n",
      "第75次迭代，权重更新为: 2.1976358890533447, (0.01 * w.grad): 0.044849708676338196, loss:  25.02874\n",
      "第76次迭代，权重更新为: 2.1536831855773926, (0.01 * w.grad): 0.04395271837711334, loss:  24.82960\n",
      "第77次迭代，权重更新为: 2.110609531402588, (0.01 * w.grad): 0.043073661625385284, loss:  24.63835\n",
      "第78次迭代，权重更新为: 2.068397283554077, (0.01 * w.grad): 0.04221218824386597, loss:  24.45467\n",
      "第79次迭代，权重更新为: 2.027029275894165, (0.01 * w.grad): 0.041367944329977036, loss:  24.27827\n",
      "第80次迭代，权重更新为: 1.986488699913025, (0.01 * w.grad): 0.040540583431720734, loss:  24.10885\n",
      "第81次迭代，权重更新为: 1.9467589855194092, (0.01 * w.grad): 0.0397297739982605, loss:  23.94614\n",
      "第82次迭代，权重更新为: 1.9078238010406494, (0.01 * w.grad): 0.03893517702817917, loss:  23.78987\n",
      "第83次迭代，权重更新为: 1.8696672916412354, (0.01 * w.grad): 0.038156475871801376, loss:  23.63979\n",
      "第84次迭代，权重更新为: 1.8322739601135254, (0.01 * w.grad): 0.037393346428871155, loss:  23.49566\n",
      "第85次迭代，权重更新为: 1.7956284284591675, (0.01 * w.grad): 0.03664547950029373, loss:  23.35723\n",
      "第86次迭代，权重更新为: 1.7597159147262573, (0.01 * w.grad): 0.03591256961226463, loss:  23.22428\n",
      "第87次迭代，权重更新为: 1.7245216369628906, (0.01 * w.grad): 0.03519431874155998, loss:  23.09660\n",
      "第88次迭代，权重更新为: 1.6900311708450317, (0.01 * w.grad): 0.0344904325902462, loss:  22.97397\n",
      "第89次迭代，权重更新为: 1.6562305688858032, (0.01 * w.grad): 0.033800624310970306, loss:  22.85620\n",
      "第90次迭代，权重更新为: 1.6231060028076172, (0.01 * w.grad): 0.03312461078166962, loss:  22.74310\n",
      "第91次迭代，权重更新为: 1.5906438827514648, (0.01 * w.grad): 0.032462120056152344, loss:  22.63447\n",
      "第92次迭代，权重更新为: 1.558830976486206, (0.01 * w.grad): 0.0318128764629364, loss:  22.53015\n",
      "第93次迭代，权重更新为: 1.5276544094085693, (0.01 * w.grad): 0.031176619231700897, loss:  22.42995\n",
      "第94次迭代，权重更新为: 1.4971013069152832, (0.01 * w.grad): 0.03055308759212494, loss:  22.33373\n",
      "第95次迭代，权重更新为: 1.4671592712402344, (0.01 * w.grad): 0.029942026361823082, loss:  22.24131\n",
      "第96次迭代，权重更新为: 1.4378161430358887, (0.01 * w.grad): 0.02934318408370018, loss:  22.15256\n",
      "第97次迭代，权重更新为: 1.409059762954712, (0.01 * w.grad): 0.02875632233917713, loss:  22.06732\n",
      "第98次迭代，权重更新为: 1.3808785676956177, (0.01 * w.grad): 0.02818119525909424, loss:  21.98545\n",
      "第99次迭代，权重更新为: 1.3532609939575195, (0.01 * w.grad): 0.027617570012807846, loss:  21.90683\n",
      "第100次迭代，权重更新为: 1.3261957168579102, (0.01 * w.grad): 0.02706521935760975, loss:  21.83132\n",
      "最终结果为: 1.3261957168579102, 梯度: 2.706521987915039, loss:  21.83132\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "需要注意的问题：自动微分不能直接转numpy数组，会报错。 需要使用detach()函数，拷贝一份张量，然后再转为numpy数组",
   "id": "e9f97358a0342e01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:30:41.611733Z",
     "start_time": "2026-01-30T03:30:41.606689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- 定义张量\n",
    "t1 = torch.tensor([10, 20], requires_grad=True, dtype=torch.float)\n",
    "print(f\"t1: {t1}, dtype: {type(t1)}\")\n",
    "\n",
    "# 2- 尝试把上述张量转为numpy数组\n",
    "# t1_numpy = t1.numpy()       # 会报错，原因是t1张量是需要计算梯度的张量，不能直接转为numpy数组\n",
    "\n",
    "# 正确做法: 使用detach()函数，拷贝一份张量，然后再转为numpy数组\n",
    "t2 = t1.detach().numpy()\n",
    "print(f\"t2: {t2}, dtype: {type(t2)}\")\n",
    "\n",
    "# 3- 测试上述的t1和t2是否共享同一块空间\n",
    "t1.data[0] = 100\n",
    "print(f\"t1: {t1}, type: {type(t2)}\")\n",
    "print(f\"t2: {t2}, type: {type(t2)}\")\n",
    "\n",
    "\n",
    "# # 4- 查看t1和t2的谁可以自动微分\n",
    "# print(f\"t1: {t1.requires_grad}\")      # True\n",
    "# print(f\"t2: {t2.requires_grad}\")      #  False"
   ],
   "id": "c9097314c50ae812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([10., 20.], requires_grad=True), dtype: <class 'torch.Tensor'>\n",
      "t2: [10. 20.], dtype: <class 'numpy.ndarray'>\n",
      "t1: tensor([100.,  20.], requires_grad=True), type: <class 'numpy.ndarray'>\n",
      "t2: [100.  20.], type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "自动微分综合demo, 模拟线性回归的训练过程",
   "id": "836311e56524428c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T03:34:38.619501Z",
     "start_time": "2026-01-30T03:34:38.419131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']        # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False          # 解决负号显示为方块的问题\n",
    "\n",
    "def create_dataset():\n",
    "    # 1- 创建数据集对象\n",
    "    x, y, coef = make_regression(\n",
    "        n_samples=100,\n",
    "        n_features=1,\n",
    "        noise=10.0,\n",
    "        coef=True,\n",
    "        bias=14.5,\n",
    "        random_state=3\n",
    "    )\n",
    "    # 2- 把上述数据封装成张量对象\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # 3- 返回结果\n",
    "    return x, y, coef\n",
    "\n",
    "def train(x, y, coef):\n",
    "    # 1- 创建数据集对象,tensor -> 数据集对象 -> 数据加载器\n",
    "    dataset = TensorDataset(x, y)\n",
    "\n",
    "    # 2- 创建数据加载器对象\n",
    "    # 参1：数据集对象  参2：批次大小  参3：是否打乱数据\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # 3- 创建初始的线性回归模型\n",
    "    # 参1：输入特征维度  参2：输出特征维度\n",
    "    model = nn.Linear(1, 1)\n",
    "\n",
    "    # 4- 创建损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 5- 创建优化器\n",
    "    # 参1：待优化的参数  参2：学习率(learning rate) -> lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # 6- 训练模型\n",
    "    # 6.1- 定义变量，分别表示训练轮数、每轮的（平均）损失值、训练总损失值、训练样本数\n",
    "    epochs, loss_list, total_loss, total_sample = 100, [], 0.0, 0\n",
    "    # 6.2- 开始训练\n",
    "    for epoch in range(epochs):\n",
    "        # 6.3- 每轮是分批次训练的，所以从数据加载器中获取数据\n",
    "        for x_train, y_train in dataloader:\n",
    "            # 6.4- 模型预测\n",
    "            y_pred = model(x_train)\n",
    "            # 6.5- 计算损失\n",
    "            loss = criterion(y_pred, y_train.reshape(-1, 1))\n",
    "            # 6.6- 计算总损失和样本批次\n",
    "            total_loss += loss.item()\n",
    "            total_sample += 1\n",
    "            # 6.7- 反向传播，计算梯度 并更新参数\n",
    "            optimizer.zero_grad()           # 梯度清零\n",
    "            loss.sum().backward()           #\n",
    "\n",
    "        # 6.8 把本轮的（平均）损失值，添加到列表中\n",
    "        loss_list.append(total_loss / total_sample)\n",
    "        print(f\"第{epoch + 1}轮迭代，总损失为: {total_loss / total_sample: .5f}\")\n",
    "\n",
    "    # 7- 打印最终结果\n",
    "    print(f\"{epochs}轮的平均损失分别为: {loss_list}\")\n",
    "\n",
    "\n",
    "# 1- 创建数据集\n",
    "x, y, coef = create_dataset()\n",
    "# 2- 训练模型\n",
    "train(x, y, coef)"
   ],
   "id": "88a43a29b88b4028",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1轮迭代，总损失为:  1021.39173\n",
      "第2轮迭代，总损失为:  1121.77366\n",
      "第3轮迭代，总损失为:  1085.40471\n",
      "第4轮迭代，总损失为:  1069.27768\n",
      "第5轮迭代，总损失为:  1114.70541\n",
      "第6轮迭代，总损失为:  1114.64967\n",
      "第7轮迭代，总损失为:  1101.68219\n",
      "第8轮迭代，总损失为:  1092.50903\n",
      "第9轮迭代，总损失为:  1085.86407\n",
      "第10轮迭代，总损失为:  1094.15986\n",
      "第11轮迭代，总损失为:  1085.37984\n",
      "第12轮迭代，总损失为:  1083.20858\n",
      "第13轮迭代，总损失为:  1082.57061\n",
      "第14轮迭代，总损失为:  1096.89361\n",
      "第15轮迭代，总损失为:  1095.66063\n",
      "第16轮迭代，总损失为:  1093.06621\n",
      "第17轮迭代，总损失为:  1088.49383\n",
      "第18轮迭代，总损失为:  1089.46782\n",
      "第19轮迭代，总损失为:  1093.28632\n",
      "第20轮迭代，总损失为:  1096.52794\n",
      "第21轮迭代，总损失为:  1094.31942\n",
      "第22轮迭代，总损失为:  1095.74910\n",
      "第23轮迭代，总损失为:  1092.99043\n",
      "第24轮迭代，总损失为:  1089.73531\n",
      "第25轮迭代，总损失为:  1095.10833\n",
      "第26轮迭代，总损失为:  1093.91439\n",
      "第27轮迭代，总损失为:  1092.59995\n",
      "第28轮迭代，总损失为:  1098.36422\n",
      "第29轮迭代，总损失为:  1097.34663\n",
      "第30轮迭代，总损失为:  1098.56992\n",
      "第31轮迭代，总损失为:  1100.58281\n",
      "第32轮迭代，总损失为:  1097.64664\n",
      "第33轮迭代，总损失为:  1095.43482\n",
      "第34轮迭代，总损失为:  1095.90933\n",
      "第35轮迭代，总损失为:  1094.19313\n",
      "第36轮迭代，总损失为:  1093.57557\n",
      "第37轮迭代，总损失为:  1093.06372\n",
      "第38轮迭代，总损失为:  1093.44713\n",
      "第39轮迭代，总损失为:  1093.35450\n",
      "第40轮迭代，总损失为:  1094.19978\n",
      "第41轮迭代，总损失为:  1093.97748\n",
      "第42轮迭代，总损失为:  1093.39051\n",
      "第43轮迭代，总损失为:  1097.04607\n",
      "第44轮迭代，总损失为:  1097.24216\n",
      "第45轮迭代，总损失为:  1098.07909\n",
      "第46轮迭代，总损失为:  1098.28084\n",
      "第47轮迭代，总损失为:  1097.31653\n",
      "第48轮迭代，总损失为:  1095.91612\n",
      "第49轮迭代，总损失为:  1094.86849\n",
      "第50轮迭代，总损失为:  1094.61375\n",
      "第51轮迭代，总损失为:  1095.57411\n",
      "第52轮迭代，总损失为:  1093.78309\n",
      "第53轮迭代，总损失为:  1095.91661\n",
      "第54轮迭代，总损失为:  1098.36220\n",
      "第55轮迭代，总损失为:  1097.52264\n",
      "第56轮迭代，总损失为:  1097.74310\n",
      "第57轮迭代，总损失为:  1096.29247\n",
      "第58轮迭代，总损失为:  1097.29043\n",
      "第59轮迭代，总损失为:  1095.77495\n",
      "第60轮迭代，总损失为:  1095.34494\n",
      "第61轮迭代，总损失为:  1097.32428\n",
      "第62轮迭代，总损失为:  1097.50027\n",
      "第63轮迭代，总损失为:  1096.15687\n",
      "第64轮迭代，总损失为:  1094.64156\n",
      "第65轮迭代，总损失为:  1094.18904\n",
      "第66轮迭代，总损失为:  1096.01717\n",
      "第67轮迭代，总损失为:  1098.15881\n",
      "第68轮迭代，总损失为:  1098.04833\n",
      "第69轮迭代，总损失为:  1097.81418\n",
      "第70轮迭代，总损失为:  1097.94892\n",
      "第71轮迭代，总损失为:  1097.13925\n",
      "第72轮迭代，总损失为:  1096.35936\n",
      "第73轮迭代，总损失为:  1099.31689\n",
      "第74轮迭代，总损失为:  1101.00592\n",
      "第75轮迭代，总损失为:  1099.69095\n",
      "第76轮迭代，总损失为:  1100.47004\n",
      "第77轮迭代，总损失为:  1102.11991\n",
      "第78轮迭代，总损失为:  1102.75847\n",
      "第79轮迭代，总损失为:  1102.91556\n",
      "第80轮迭代，总损失为:  1103.75733\n",
      "第81轮迭代，总损失为:  1106.12946\n",
      "第82轮迭代，总损失为:  1105.35594\n",
      "第83轮迭代，总损失为:  1106.66100\n",
      "第84轮迭代，总损失为:  1106.77598\n",
      "第85轮迭代，总损失为:  1107.00224\n",
      "第86轮迭代，总损失为:  1106.88334\n",
      "第87轮迭代，总损失为:  1107.37505\n",
      "第88轮迭代，总损失为:  1106.32561\n",
      "第89轮迭代，总损失为:  1106.42634\n",
      "第90轮迭代，总损失为:  1107.65304\n",
      "第91轮迭代，总损失为:  1107.32601\n",
      "第92轮迭代，总损失为:  1106.59045\n",
      "第93轮迭代，总损失为:  1107.18193\n",
      "第94轮迭代，总损失为:  1107.80324\n",
      "第95轮迭代，总损失为:  1107.08246\n",
      "第96轮迭代，总损失为:  1109.19084\n",
      "第97轮迭代，总损失为:  1109.77903\n",
      "第98轮迭代，总损失为:  1110.79468\n",
      "第99轮迭代，总损失为:  1111.97084\n",
      "第100轮迭代，总损失为:  1111.56269\n",
      "100轮的平均损失分别为: [1021.3917323521206, 1121.7736598423548, 1085.4047117687408, 1069.2776832580566, 1114.7054072788783, 1114.6496738252185, 1101.6821921212334, 1092.509030205863, 1085.8640657455203, 1094.1598587036133, 1085.3798414948699, 1083.2085846492223, 1082.5706075312016, 1096.8936069254974, 1095.6606310889833, 1093.0662083625793, 1088.4938266657982, 1089.4678223019555, 1093.2863158606049, 1096.5279353005546, 1094.3194218330643, 1095.7490961149142, 1092.990432407545, 1089.7353140967232, 1095.1083258492606, 1093.9143901028476, 1092.5999526170197, 1098.3642232077461, 1097.346626732737, 1098.5699166070847, 1100.5828097646686, 1097.6466410160065, 1095.4348231394054, 1095.90932737078, 1094.193131520797, 1093.5755727556016, 1093.0637193775544, 1093.44712757885, 1093.3545016613634, 1094.1997820990425, 1093.9774795957558, 1093.3905135952696, 1097.0460700608567, 1097.2421616145543, 1098.0790907844664, 1098.2808444337074, 1097.3165313581565, 1095.916115238553, 1094.8684948401271, 1094.6137477111815, 1095.5741134130653, 1093.7830937773317, 1095.916609833504, 1098.362199066808, 1097.52263673064, 1097.743100828054, 1096.2924745602716, 1097.2904281804126, 1095.774954123878, 1095.344940676008, 1097.3242839840034, 1097.5002710544568, 1096.1568652821236, 1094.6415641222682, 1094.189041867099, 1096.0171659643, 1098.158812850269, 1098.048325634804, 1097.8141779218402, 1097.9489230642514, 1097.1392545565752, 1096.359364971282, 1099.31689383699, 1101.0059246740745, 1099.6909547860282, 1100.4700406511924, 1102.1199069828183, 1102.7584725613997, 1102.9155626831607, 1103.7573291437966, 1106.129456447756, 1105.3559368479127, 1106.6609963697738, 1106.7759840764156, 1107.0022394452776, 1106.883342571829, 1107.375047824653, 1106.3256070706752, 1106.426335140369, 1107.6530397142683, 1107.3260064177462, 1106.5904520342808, 1107.1819284735004, 1107.8032412253615, 1107.082464387363, 1109.1908418734868, 1109.779027042754, 1110.794679530508, 1111.970843795402, 1111.5626930618287]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f1ba28236d4da8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
